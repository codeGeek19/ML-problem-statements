{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":125192,"databundleVersionId":15408205,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:56.054565Z","iopub.execute_input":"2026-02-16T19:18:56.054875Z","iopub.status.idle":"2026-02-16T19:18:56.062467Z","shell.execute_reply.started":"2026-02-16T19:18:56.054850Z","shell.execute_reply":"2026-02-16T19:18:56.061293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, FunctionTransformer, OneHotEncoder, LabelEncoder, KBinsDiscretizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom xgboost import XGBClassifier\nfrom collections import Counter\nfrom scipy import sparse\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_is_fitted\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:56.074436Z","iopub.execute_input":"2026-02-16T19:18:56.074746Z","iopub.status.idle":"2026-02-16T19:18:56.090574Z","shell.execute_reply.started":"2026-02-16T19:18:56.074721Z","shell.execute_reply":"2026-02-16T19:18:56.089419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s6e2/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s6e2/test.csv\")\nsample_submit = pd.read_csv(\"/kaggle/input/playground-series-s6e2/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:56.092220Z","iopub.execute_input":"2026-02-16T19:18:56.092581Z","iopub.status.idle":"2026-02-16T19:18:57.117799Z","shell.execute_reply.started":"2026-02-16T19:18:56.092542Z","shell.execute_reply":"2026-02-16T19:18:57.116543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:57.119057Z","iopub.execute_input":"2026-02-16T19:18:57.119430Z","iopub.status.idle":"2026-02-16T19:18:57.129591Z","shell.execute_reply.started":"2026-02-16T19:18:57.119388Z","shell.execute_reply":"2026-02-16T19:18:57.128745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:57.130766Z","iopub.execute_input":"2026-02-16T19:18:57.131179Z","iopub.status.idle":"2026-02-16T19:18:57.158656Z","shell.execute_reply.started":"2026-02-16T19:18:57.131148Z","shell.execute_reply":"2026-02-16T19:18:57.157403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submit[\"Heart Disease\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:57.161657Z","iopub.execute_input":"2026-02-16T19:18:57.162847Z","iopub.status.idle":"2026-02-16T19:18:57.183881Z","shell.execute_reply.started":"2026-02-16T19:18:57.162765Z","shell.execute_reply":"2026-02-16T19:18:57.182570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CAT_COLUMNS = [\n    \"Chest pain type\",\n    \"EKG results\",\n    \"Slope of ST\",\n    \"Number of vessels fluro\",\"Thallium\", \n    \"Sex\",\n    \"FBS over 120\",\n    \"Exercise angina\"\n]\n\nNUM_COLUMNS = [\n    \"Age\",\n    \"BP\",\n    \"Cholesterol\",\n    \"Max HR\",\n    \"ST depression\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:57.185149Z","iopub.execute_input":"2026-02-16T19:18:57.185531Z","iopub.status.idle":"2026-02-16T19:18:57.207825Z","shell.execute_reply.started":"2026-02-16T19:18:57.185496Z","shell.execute_reply":"2026-02-16T19:18:57.206603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.drop(\"id\", axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:57.209037Z","iopub.execute_input":"2026-02-16T19:18:57.209520Z","iopub.status.idle":"2026-02-16T19:18:57.254956Z","shell.execute_reply.started":"2026-02-16T19:18:57.209310Z","shell.execute_reply":"2026-02-16T19:18:57.253921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"le = LabelEncoder()\ntrain_with_num_label = train.copy()\ntrain_with_num_label[\"Heart Disease\"] = le.fit_transform(train_with_num_label[\"Heart Disease\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:57.256591Z","iopub.execute_input":"2026-02-16T19:18:57.257025Z","iopub.status.idle":"2026-02-16T19:18:57.407077Z","shell.execute_reply.started":"2026-02-16T19:18:57.256996Z","shell.execute_reply":"2026-02-16T19:18:57.405894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nsns.heatmap(train_with_num_label.corr(), annot=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:57.408272Z","iopub.execute_input":"2026-02-16T19:18:57.408657Z","iopub.status.idle":"2026-02-16T19:18:58.597236Z","shell.execute_reply.started":"2026-02-16T19:18:57.408630Z","shell.execute_reply":"2026-02-16T19:18:58.595267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(1, len(NUM_COLUMNS), figsize=(10, 5))\nfor i in range(len(NUM_COLUMNS)):\n    ax[i].boxplot(train_with_num_label[NUM_COLUMNS[i]])\n    ax[i].set_title(NUM_COLUMNS[i] + \" dist\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:58.598926Z","iopub.execute_input":"2026-02-16T19:18:58.599449Z","iopub.status.idle":"2026-02-16T19:18:59.316961Z","shell.execute_reply.started":"2026-02-16T19:18:58.599287Z","shell.execute_reply":"2026-02-16T19:18:59.315759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train[\"Cholesterol\"] = np.clip(train[\"Cholesterol\"], None, train[\"Cholesterol\"].quantile(0.999))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:59.318723Z","iopub.execute_input":"2026-02-16T19:18:59.319083Z","iopub.status.idle":"2026-02-16T19:18:59.341646Z","shell.execute_reply.started":"2026-02-16T19:18:59.319044Z","shell.execute_reply":"2026-02-16T19:18:59.340515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FrequencyEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, output_sparse=True, dtype=float, unknown_value=0.0):\n        self.output_sparse = output_sparse\n        self.dtype = dtype\n        self.unknown_value = unknown_value\n\n    def fit(self, X, y=None):\n        X = self._to_2d_object(X)\n        self.n_features_in_ = X.shape[1]\n\n        self._maps_ = []\n        n = X.shape[0]\n        for j in range(self.n_features_in_):\n            cnt = Counter(X[:, j].tolist())\n            self._maps_.append({k: (v / n) for k, v in cnt.items()})\n        return self\n\n    def transform(self, X):\n        check_is_fitted(self, attributes=[\"_maps_\", \"n_features_in_\"])\n        X = self._to_2d_object(X)\n\n        if X.shape[1] != self.n_features_in_:\n            raise ValueError(f\"Expected {self.n_features_in_} features, got {X.shape[1]}\")\n\n        out = np.empty((X.shape[0], self.n_features_in_), dtype=self.dtype)\n        for j, mp in enumerate(self._maps_):\n            out[:, j] = np.fromiter((mp.get(v, self.unknown_value) for v in X[:, j]), dtype=self.dtype)\n\n        return sparse.csr_matrix(out) if self.output_sparse else out\n\n    def get_feature_names_out(self, input_features=None):\n        if input_features is None:\n            input_features = [f\"x{i}\" for i in range(getattr(self, \"n_features_in_\", 0))]\n        return np.asarray([f\"freq__{f}\" for f in input_features], dtype=object)\n\n    @staticmethod\n    def _to_2d_object(X):\n        if hasattr(X, \"to_numpy\"):\n            X = X.to_numpy()\n        X = np.asarray(X, dtype=object)\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n        return X\n\n\nBIN_COLUMNS = [\n    \"Max HR\",\n    \"Age\",\n    # \"ST depression\"\n]\nNUM_CONT_COLUMNS = NUM_COLUMNS\n\nxgb_es_rounds = 156\n\n\nnum_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n    (\"scaler\", StandardScaler()),\n])\n\nbin_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"bins\", KBinsDiscretizer(\n        n_bins=10,\n        encode=\"onehot\",\n        strategy=\"quantile\",\n    )),\n])\n\ncat_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"enc\", FeatureUnion(transformer_list=[\n        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)),\n        (\"freq\", FrequencyEncoder(output_sparse=True)),\n    ])),\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", num_pipe, NUM_CONT_COLUMNS),\n        # (\"num_bins\", bin_pipe, BIN_COLUMNS),\n        (\"cat\", cat_pipe, CAT_COLUMNS),\n    ],\n    remainder=\"drop\",\n    verbose_feature_names_out=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:59.343175Z","iopub.execute_input":"2026-02-16T19:18:59.343568Z","iopub.status.idle":"2026-02-16T19:18:59.360396Z","shell.execute_reply.started":"2026-02-16T19:18:59.343539Z","shell.execute_reply":"2026-02-16T19:18:59.359091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cb_params = dict(\n    loss_function=\"Logloss\",\n    random_seed=42,\n    allow_writing_files=False,\n    verbose=False,\n\n    iterations=3655,\n    depth=4,\n    learning_rate=0.033382081130147116,\n    l2_leaf_reg=0.893029919896381,\n    min_data_in_leaf=23,\n    border_count=151,\n\n    od_type=\"Iter\",\n    od_wait=231,\n\n)\nxgb_params = dict(\n    base_score=None, booster=None, callbacks=[],\n    colsample_bylevel=np.float64(0.9507904281995785),\n    colsample_bynode=None, colsample_bytree=1.0, device=None,\n    early_stopping_rounds=None, enable_categorical=False,\n    eval_metric=None, feature_types=None, feature_weights=None,\n    gamma=None, grow_policy=None, importance_type=None,\n    interaction_constraints=None,\n    learning_rate=np.float64(0.1393804049974062), max_bin=None,\n    max_cat_threshold=None, max_cat_to_onehot=None,\n    max_delta_step=None, max_depth=2, max_leaves=None,\n    min_child_weight=np.float64(0.9933137882303046), missing=np.nan,\n    monotone_constraints=None, multi_strategy=None, n_estimators=1699,\n    n_jobs=-1, num_parallel_tree=None\n)\n\nlgb_params = {'n_estimators': 2398, 'num_leaves': 4, 'min_child_samples': 62, 'learning_rate': np.float64(0.08242393539659958), 'log_max_bin': 9, 'colsample_bytree': np.float64(0.35659474934153196), 'reg_alpha': np.float64(0.0012278801488545012), 'reg_lambda': np.float64(2.988747127151322)}\nmodel_cb = Pipeline(steps=[\n    # (\"prep\", cat_pipe),\n    (\"clf\", CatBoostClassifier(\n        **cb_params\n    )),\n])\n\nmodel_xgb = Pipeline(steps=[\n    # (\"prep\", preprocess),\n    (\"clf\", XGBClassifier(\n        **xgb_params\n    )),\n])\n\nlgbm = Pipeline(steps=[\n    # (\"prep\", preprocess),\n    (\"clf\", LGBMClassifier(\n        **lgb_params\n    )),\n])\n\nensemble = VotingClassifier(\n    estimators=[\n        (\"cb\", model_cb),\n        (\"xgb\", model_xgb),\n        (\"lgbm\", lgbm)\n    ],\n    voting=\"soft\",\n    n_jobs=1,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:59.363666Z","iopub.execute_input":"2026-02-16T19:18:59.364268Z","iopub.status.idle":"2026-02-16T19:18:59.390148Z","shell.execute_reply.started":"2026-02-16T19:18:59.364236Z","shell.execute_reply":"2026-02-16T19:18:59.389111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ensemble.fit(train.drop(\"Heart Disease\", axis=1), train[\"Heart Disease\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:18:59.391439Z","iopub.execute_input":"2026-02-16T19:18:59.391859Z","iopub.status.idle":"2026-02-16T19:24:13.707128Z","shell.execute_reply.started":"2026-02-16T19:18:59.391798Z","shell.execute_reply":"2026-02-16T19:24:13.706276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport optuna\nimport xgboost as xgb\n\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import VotingClassifier\n\nfrom catboost import CatBoostClassifier, CatBoostError\nfrom xgboost import XGBClassifier\n\n\nSEED = 42\nN_SPLITS = 5\nTARGET_COL = \"Heart Disease\"   # <-- change if needed\nUSE_GPU = False                # <-- True if you want GPU\n\n\ndef to_binary_target(y: pd.Series) -> np.ndarray:\n    if pd.api.types.is_bool_dtype(y):\n        return y.astype(np.int32).to_numpy()\n    if pd.api.types.is_numeric_dtype(y):\n        yy = y.to_numpy()\n        uniq = np.unique(yy[~pd.isna(yy)])\n        if len(uniq) == 2 and set(uniq.tolist()) <= {0, 1}:\n            return yy.astype(np.int32)\n        return (yy == np.max(uniq)).astype(np.int32)\n    s = y.astype(str).str.strip().str.lower()\n    if set(s.unique()) <= {\"yes\", \"no\"}:\n        return (s == \"yes\").astype(np.int32).to_numpy()\n    if set(s.unique()) <= {\"true\", \"false\"}:\n        return (s == \"true\").astype(np.int32).to_numpy()\n    codes, uniques = pd.factorize(y, sort=True)\n    if len(uniques) != 2:\n        raise ValueError(f\"Expected binary target, got {len(uniques)} classes: {list(uniques)}\")\n    return codes.astype(np.int32)\n\n\ndef build_cb(trial: optuna.Trial) -> CatBoostClassifier:\n    params = dict(\n        loss_function=\"Logloss\",\n        random_seed=SEED,\n        allow_writing_files=False,\n        verbose=False,\n\n        iterations=trial.suggest_int(\"cb_iterations\", 800, 4000),\n        depth=trial.suggest_int(\"cb_depth\", 4, 10),\n        learning_rate=trial.suggest_float(\"cb_lr\", 1e-2, 0.2, log=True),\n        l2_leaf_reg=trial.suggest_float(\"cb_l2\", 1e-2, 20.0, log=True),\n\n        min_data_in_leaf=trial.suggest_int(\"cb_min_data_in_leaf\", 1, 128, log=True),\n        border_count=trial.suggest_int(\"cb_border_count\", 32, 255),\n\n        od_type=\"Iter\",\n        od_wait=trial.suggest_int(\"cb_od_wait\", 50, 400),\n    )\n\n    # IMPORTANT FIX:\n    # keep cb_rsm ALWAYS in trial params (so best_params always has it),\n    # but pass it to CatBoost ONLY on CPU (GPU may conflict with rsm).\n    cb_rsm = trial.suggest_float(\"cb_rsm\", 0.5, 1.0)\n\n    if USE_GPU:\n        params.update(dict(task_type=\"GPU\", devices=\"0\"))\n        # do NOT pass rsm on GPU\n    else:\n        params.update(dict(task_type=\"CPU\", thread_count=-1, rsm=cb_rsm))\n\n    return CatBoostClassifier(**params)\n\n\ndef build_xgb(trial: optuna.Trial) -> XGBClassifier:\n    params = dict(\n        n_estimators=trial.suggest_int(\"xgb_n_estimators\", 800, 6000),\n        learning_rate=trial.suggest_float(\"xgb_lr\", 5e-3, 0.2, log=True),\n        max_depth=trial.suggest_int(\"xgb_max_depth\", 3, 12),\n        min_child_weight=trial.suggest_float(\"xgb_min_child_weight\", 1e-2, 50.0, log=True),\n        subsample=trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n        colsample_bytree=trial.suggest_float(\"xgb_colsample\", 0.5, 1.0),\n        reg_lambda=trial.suggest_float(\"xgb_reg_lambda\", 1e-3, 50.0, log=True),\n        reg_alpha=trial.suggest_float(\"xgb_reg_alpha\", 1e-8, 10.0, log=True),\n        gamma=trial.suggest_float(\"xgb_gamma\", 0.0, 10.0),\n\n        eval_metric=\"logloss\",\n        random_state=SEED,\n        n_jobs=-1,\n        tree_method=\"hist\",\n    )\n    if USE_GPU:\n        params.update(dict(device=\"cuda\"))\n    return XGBClassifier(**params)\n\n\ndef fit_xgb_with_es(model: XGBClassifier, Xtr, ytr, Xva, yva, rounds: int):\n    \"\"\"\n    Compatible early stopping across xgboost versions:\n      1) try early_stopping_rounds in fit\n      2) fallback to callbacks=[xgb.callback.EarlyStopping(...)]\n      3) fallback to plain fit without early stopping\n    \"\"\"\n    try:\n        model.fit(\n            Xtr, ytr,\n            eval_set=[(Xva, yva)],\n            verbose=False,\n            early_stopping_rounds=rounds,\n        )\n        return\n    except TypeError:\n        pass\n\n    try:\n        cb = [xgb.callback.EarlyStopping(rounds=rounds, save_best=True)]\n        model.fit(\n            Xtr, ytr,\n            eval_set=[(Xva, yva)],\n            verbose=False,\n            callbacks=cb,\n        )\n        return\n    except TypeError:\n        model.fit(\n            Xtr, ytr,\n            eval_set=[(Xva, yva)],\n            verbose=False,\n        )\n\n\ndef xgb_predict_proba_best(model: XGBClassifier, X):\n    \"\"\"\n    Try to use best_iteration if present (varies by xgboost version).\n    \"\"\"\n    bi = None\n    for attr in (\"best_iteration\", \"best_iteration_\"):\n        if hasattr(model, attr):\n            bi = getattr(model, attr)\n            if bi is not None:\n                break\n\n    if bi is not None:\n        bi = int(bi)\n        # new API\n        try:\n            return model.predict_proba(X, iteration_range=(0, bi + 1))\n        except TypeError:\n            pass\n        # older API\n        try:\n            return model.predict_proba(X, ntree_limit=bi + 1)\n        except TypeError:\n            pass\n\n    return model.predict_proba(X)\n\n\ndef objective(trial: optuna.Trial, train: pd.DataFrame, preprocess):\n    X = train.drop(TARGET_COL, axis=1)\n    y = to_binary_target(train[TARGET_COL])\n\n    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n    w_cb = trial.suggest_float(\"ens_w_cb\", 0.1, 0.9)\n    xgb_es = trial.suggest_int(\"xgb_es_rounds\", 30, 300)\n\n    scores = []\n    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y[tr_idx], y[va_idx]\n\n        # preprocess fit on train only\n        prep = clone(preprocess)\n        Xtr_t = prep.fit_transform(X_tr)\n        Xva_t = prep.transform(X_va)\n\n        # CatBoost\n        cb = build_cb(trial)\n        cb.fit(\n            Xtr_t, y_tr,\n            eval_set=(Xva_t, y_va),\n            use_best_model=True,\n        )\n        p_cb = cb.predict_proba(Xva_t)[:, 1]\n\n        # XGBoost (compat ES)\n        xgbm = build_xgb(trial)\n        fit_xgb_with_es(xgbm, Xtr_t, y_tr, Xva_t, y_va, rounds=xgb_es)\n        p_xgb = xgb_predict_proba_best(xgbm, Xva_t)[:, 1]\n\n        p = w_cb * p_cb + (1.0 - w_cb) * p_xgb\n        auc = roc_auc_score(y_va, p)\n        scores.append(auc)\n\n        trial.report(float(np.mean(scores)), step=fold)\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n\n        del prep, Xtr_t, Xva_t, cb, xgbm, p_cb, p_xgb, p\n        gc.collect()\n\n    return float(np.mean(scores))\n\n\ndef run_optuna(train: pd.DataFrame, preprocess, n_trials: int = 50):\n    sampler = optuna.samplers.TPESampler(seed=SEED)\n    pruner = optuna.pruners.MedianPruner(n_warmup_steps=2)\n    study = optuna.create_study(direction=\"maximize\", sampler=sampler, pruner=pruner)\n\n    study.optimize(\n        lambda t: objective(t, train, preprocess),\n        n_trials=n_trials,\n        gc_after_trial=True,\n        catch=(CatBoostError, ValueError),\n    )\n\n    print(\"Best AUC:\", study.best_value)\n    print(\"Best params:\", study.best_params)\n    return study\n\n\ndef build_ensemble_from_best(best_params: dict, preprocess):\n    # --- CatBoost ---\n    cb_kwargs = dict(\n        verbose=200,\n        loss_function=\"Logloss\",\n        random_seed=SEED,\n        allow_writing_files=False,\n\n        iterations=int(best_params[\"cb_iterations\"]),\n        depth=int(best_params[\"cb_depth\"]),\n        learning_rate=float(best_params[\"cb_lr\"]),\n        l2_leaf_reg=float(best_params[\"cb_l2\"]),\n        min_data_in_leaf=int(best_params[\"cb_min_data_in_leaf\"]),\n        border_count=int(best_params[\"cb_border_count\"]),\n        od_type=\"Iter\",\n        od_wait=int(best_params[\"cb_od_wait\"]),\n    )\n\n    if USE_GPU:\n        cb_kwargs.update(dict(task_type=\"GPU\", devices=\"0\"))\n        # rsm not passed on GPU\n    else:\n        cb_kwargs.update(dict(task_type=\"CPU\", thread_count=-1))\n        cb_kwargs[\"rsm\"] = float(best_params.get(\"cb_rsm\", 1.0))  # SAFE fallback\n\n    cb = CatBoostClassifier(**cb_kwargs)\n\n    model_cb = Pipeline(steps=[\n        (\"prep\", clone(preprocess)),\n        (\"clf\", cb),\n    ])\n\n    # --- XGBoost ---\n    xgbm = XGBClassifier(\n        n_estimators=int(best_params[\"xgb_n_estimators\"]),\n        learning_rate=float(best_params[\"xgb_lr\"]),\n        max_depth=int(best_params[\"xgb_max_depth\"]),\n        min_child_weight=float(best_params[\"xgb_min_child_weight\"]),\n        subsample=float(best_params[\"xgb_subsample\"]),\n        colsample_bytree=float(best_params[\"xgb_colsample\"]),\n        reg_lambda=float(best_params[\"xgb_reg_lambda\"]),\n        reg_alpha=float(best_params[\"xgb_reg_alpha\"]),\n        gamma=float(best_params[\"xgb_gamma\"]),\n        eval_metric=\"logloss\",\n        random_state=SEED,\n        n_jobs=-1,\n        tree_method=\"hist\",\n        **({\"device\": \"cuda\"} if USE_GPU else {}),\n    )\n\n    model_xgb = Pipeline(steps=[\n        (\"prep\", clone(preprocess)),\n        (\"clf\", xgbm),\n    ])\n\n    w_cb = float(best_params[\"ens_w_cb\"])\n\n    ensemble = VotingClassifier(\n        estimators=[(\"cb\", model_cb), (\"xgb\", model_xgb)],\n        voting=\"soft\",\n        weights=[w_cb, 1.0 - w_cb],\n        n_jobs=1,\n    )\n    return ensemble\n\n\n# =========================\n# Usage\n# =========================\n# study = run_optuna(train, preprocess, n_trials=50)\n# ensemble = build_ensemble_from_best(study.best_params, preprocess)\n\n# If you already have best_params:\nbest_params = {\n    'ens_w_cb': 0.8402766722774058,\n    'xgb_es_rounds': 156,\n    'cb_iterations': 3655,\n    'cb_depth': 4,\n    'cb_lr': 0.033382081130147116,\n    'cb_l2': 0.893029919896381,\n    'cb_min_data_in_leaf': 23,\n    'cb_border_count': 151,\n    'cb_od_wait': 231,\n    # 'cb_rsm': 0.9,  # optional; safe if missing\n    'xgb_n_estimators': 3817,\n    'xgb_lr': 0.10026984626576654,\n    'xgb_max_depth': 6,\n    'xgb_min_child_weight': 4.749437616484939,\n    'xgb_subsample': 0.5485378859389886,\n    'xgb_colsample': 0.8271587149072527,\n    'xgb_reg_lambda': 4.939831905315371,\n    'xgb_reg_alpha': 0.0004507365169724808,\n    'xgb_gamma': 5.9043357702728105\n}\n\n# ensemble = build_ensemble_from_best(best_params, preprocess)\n# ensemble.fit(train.drop(TARGET_COL, axis=1), train[TARGET_COL])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:24:13.708772Z","iopub.execute_input":"2026-02-16T19:24:13.709158Z","iopub.status.idle":"2026-02-16T19:24:13.745675Z","shell.execute_reply.started":"2026-02-16T19:24:13.709128Z","shell.execute_reply":"2026-02-16T19:24:13.744616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.drop(\"id\", inplace=True, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:24:13.746834Z","iopub.execute_input":"2026-02-16T19:24:13.747342Z","iopub.status.idle":"2026-02-16T19:24:13.781649Z","shell.execute_reply.started":"2026-02-16T19:24:13.747255Z","shell.execute_reply":"2026-02-16T19:24:13.780611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = ensemble.predict_proba(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:24:13.783234Z","iopub.execute_input":"2026-02-16T19:24:13.783689Z","iopub.status.idle":"2026-02-16T19:24:31.568861Z","shell.execute_reply.started":"2026-02-16T19:24:13.783652Z","shell.execute_reply":"2026-02-16T19:24:31.567711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:24:31.570051Z","iopub.execute_input":"2026-02-16T19:24:31.570576Z","iopub.status.idle":"2026-02-16T19:24:31.577538Z","shell.execute_reply.started":"2026-02-16T19:24:31.570543Z","shell.execute_reply":"2026-02-16T19:24:31.576574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submit[\"Heart Disease\"] = preds[:, 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:24:31.578710Z","iopub.execute_input":"2026-02-16T19:24:31.579507Z","iopub.status.idle":"2026-02-16T19:24:31.598122Z","shell.execute_reply.started":"2026-02-16T19:24:31.579411Z","shell.execute_reply":"2026-02-16T19:24:31.596868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submit.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T19:28:02.137006Z","iopub.execute_input":"2026-02-16T19:28:02.137392Z","iopub.status.idle":"2026-02-16T19:28:02.154813Z","shell.execute_reply.started":"2026-02-16T19:28:02.137360Z","shell.execute_reply":"2026-02-16T19:28:02.153415Z"}},"outputs":[],"execution_count":null}]}